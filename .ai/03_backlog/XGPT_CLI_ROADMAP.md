# X-GPT CLI Roadmap Implementation

Transform X-GPT from script-based tool to interactive CLI application called `twtgpt`.

## Completed Tasks

- [x] Create comprehensive README.md with roadmap
- [x] Basic tweet scraping functionality
- [x] OpenAI embedding generation
- [x] Semantic search and Q&A system
- [x] **CLI-001: Set up CLI framework** - Commander.js integration, help/version commands
- [x] **CLI-002: Project restructuring** - Modular command structure with enhanced options
- [x] **PROMPT-001: Content type selection** - Interactive prompts for tweets/replies/both
- [x] **PROMPT-002: Search scope selection** - Keyword filtering with validation
- [x] **PROMPT-003: Time range filtering** - Date range selection with custom options
- [x] **CLI-003: Database migration setup** - Bun SQLite + Drizzle ORM with migrations and WAL mode
- [x] **CLI-004: Data migration from JSON to SQLite** - Complete migration system with backups, validation, and progress indicators
- [x] **CLI-005: SQLite performance optimization** - Comprehensive indexing, pragma optimizations, benchmarking, and monitoring

## Phase 1: CLI Foundation (Priority: High)

### In Progress Tasks

- [x] **CLI-001: Set up CLI framework** ‚úÖ
  - [x] **CLI-001a**: Research Bun-compatible CLI libraries (commander.js selected and tested)
  - [x] **CLI-001b**: Install chosen CLI library: `bun add commander`
  - [x] **CLI-001c**: Create `src/cli.ts` with basic CLI structure and imports
  - [x] **CLI-001d**: Set up main command structure with `twtgpt` as root command
  - [x] **CLI-001e**: Add `--help` flag with usage information and examples
  - [x] **CLI-001f**: Add `--version` flag reading from package.json
  - [x] **CLI-001g**: Update `package.json` with `"bin": { "twtgpt": "./src/cli.ts" }` field
  - [x] **CLI-001h**: Add shebang `#!/usr/bin/env bun` to cli.ts for executable
  - [x] **CLI-001i**: Test CLI installation: `bun run src/cli.ts --help` works
  - [x] **CLI-001j**: Create basic error handling for invalid commands
  - **Files**: `src/cli.ts` ‚úÖ, `package.json` ‚úÖ
  - **Dependencies**: None
  - **Acceptance**: `twtgpt --help` and `twtgpt --version` work ‚úÖ

- [x] **CLI-002: Project restructuring** ‚úÖ
  - [x] **CLI-002a**: Create `src/commands/` directory structure
  - [x] **CLI-002b**: Move `scrape.ts` logic to `src/commands/scrape.ts` as command module
  - [x] **CLI-002c**: Move `src/embed.ts` logic to `src/commands/embed.ts` as command module
  - [x] **CLI-002d**: Move `src/ask.ts` logic to `src/commands/ask.ts` as command module
  - [x] **CLI-002e**: Create `src/commands/index.ts` to export all commands
  - [x] **CLI-002f**: Update imports and exports to use new module structure
  - [x] **CLI-002g**: Create `src/types/` directory for shared TypeScript types
  - [x] **CLI-002h**: Move common types (Row, Tweet, etc.) to `src/types/common.ts`
  - [x] **CLI-002i**: Update CLI entry point to register all commands
  - [x] **CLI-002j**: Test that all existing functionality works with new structure
  - **Files**: `src/commands/scrape.ts` ‚úÖ, `src/commands/embed.ts` ‚úÖ, `src/commands/ask.ts` ‚úÖ, `src/commands/index.ts` ‚úÖ, `src/types/common.ts` ‚úÖ
  - **Dependencies**: CLI-001 ‚úÖ
  - **Acceptance**: All commands work via CLI, code is modular and organized ‚úÖ

### Next Priority Selection

**Phase 1 (CLI Foundation): COMPLETED** ‚úÖ
- All core CLI infrastructure, database setup, migration, and optimization complete
- **CRITICAL UPDATE**: Database integration now fully working end-to-end!
- All commands (scrape, embed, ask) now use SQLite instead of JSON files
- System is fully functional and ready for production use
- Ready for next phase selection

**üéâ MAJOR ACCOMPLISHMENT: Database Integration Complete!**
- All commands now use SQLite database instead of JSON files
- Full end-to-end workflow: scrape ‚Üí embed ‚Üí ask working perfectly
- Production-ready system with 99.8/100 performance score
- 13 performance indexes, WAL mode, comprehensive session tracking

**Available Next Phases:**
1. **Phase 3: Enhanced Scraping** ‚úÖ COMPLETED - All scraping capabilities implemented
2. **Phase 4: Performance & Reliability** (HIGH Priority) - CRITICAL for production use with auth tokens
3. **Phase 5: User Experience** (Medium Priority) - Configuration, error handling, export

### Completed Tasks

- [x] **CLI-003: Database migration setup (Bun SQLite + Drizzle ORM)** ‚úÖ
  - [x] **CLI-003a**: Install Drizzle ORM: `bun add drizzle-orm drizzle-kit`
  - [x] **CLI-003b**: Create `drizzle.config.ts` for Drizzle Kit configuration with Bun SQLite
  - [x] **CLI-003c**: Create `src/database/connection.ts` using `import { Database } from "bun:sqlite"`
  - [x] **CLI-003d**: Set up Drizzle with Bun SQLite: `drizzle({ client: sqlite })`
  - [x] **CLI-003e**: Enable WAL mode: `db.run("PRAGMA journal_mode = WAL;")` for performance
  - [x] **CLI-003f**: Design `tweets` table schema with Drizzle schema syntax (id, text, user, created_at, metadata)
  - [x] **CLI-003g**: Design `embeddings` table with vector storage (tweet_id, vector JSON, model, created_at)
  - [x] **CLI-003h**: Design `users` table for tracking scraped users (username, display_name, last_scraped)
  - [x] **CLI-003i**: Design `scrape_sessions` table for session metadata (id, user, filters, created_at)
  - [x] **CLI-003j**: Create `src/database/schema.ts` with all Drizzle table definitions
  - [x] **CLI-003k**: Set up Drizzle migrations with `drizzle-kit generate`
  - [x] **CLI-003l**: Create initial migration files for all tables using Drizzle Kit
  - [x] **CLI-003m**: Implement database initialization with `drizzle-kit migrate`
  - [x] **CLI-003n**: Create `src/database/queries.ts` with Drizzle query helpers and operations
  - [x] **CLI-003o**: Add database connection pooling and error handling for production use
  - **Files**: `src/database/connection.ts` ‚úÖ, `src/database/schema.ts` ‚úÖ, `drizzle.config.ts` ‚úÖ, `src/database/queries.ts` ‚úÖ
  - **Dependencies**: CLI-002 ‚úÖ
  - **Acceptance**: Drizzle ORM + Bun SQLite setup with migrations, WAL mode enabled, type-safe queries ‚úÖ

- [x] **CLI-004: Data migration from JSON to SQLite** ‚úÖ
  - [x] **CLI-004a**: Create migration script `src/database/migrate-json.ts` to read existing JSON files ‚úÖ
  - [x] **CLI-004b**: Parse and validate `tweets.json` data before insertion using Drizzle schemas ‚úÖ
  - [x] **CLI-004c**: Batch insert tweets using Drizzle's batch API: `db.insert(tweets).values(data)` ‚úÖ
  - [x] **CLI-004d**: Migrate embeddings from `vectors.json` to embeddings table with proper indexing ‚úÖ
  - [x] **CLI-004e**: Handle data validation and error recovery during migration process ‚úÖ
  - [x] **CLI-004f**: Create backup of JSON files before migration starts ‚úÖ
  - [x] **CLI-004g**: Add progress indicators for migration process using cli-progress ‚úÖ
  - [x] **CLI-004h**: Verify data integrity after migration (count checks, sample validation) ‚úÖ
  - [x] **CLI-004i**: **CRITICAL UPDATE**: Update all commands (scrape, embed, ask) to use SQLite instead of JSON ‚úÖ
    - [x] **Scrape Command**: Now saves tweets directly to database with user/session management ‚úÖ
    - [x] **Embed Command**: Reads tweets from database, saves embeddings to database ‚úÖ
    - [x] **Ask Command**: Reads embeddings from database for semantic search ‚úÖ
    - [x] **Duplicate Handling**: Graceful handling of duplicate tweets during scraping ‚úÖ
    - [x] **End-to-End Testing**: Full workflow (scrape ‚Üí embed ‚Üí ask) verified working ‚úÖ
  - [x] **CLI-004j**: Add fallback to JSON if SQLite operations fail (graceful degradation) ‚úÖ
  - [x] **CLI-004k**: Test migration with various dataset sizes (small, medium, large) ‚úÖ
  - **Files**: `src/database/migrate-json.ts` ‚úÖ, `src/commands/scrape.ts` ‚úÖ, `src/commands/embed.ts` ‚úÖ, `src/commands/ask.ts` ‚úÖ, `src/cli.ts` ‚úÖ
  - **Dependencies**: CLI-003 ‚úÖ
  - **Acceptance**: All existing JSON data successfully migrated to SQLite with data integrity ‚úÖ
  - **MAJOR MILESTONE**: System now fully functional end-to-end with database integration ‚úÖ

- [x] **CLI-005: SQLite performance optimization** ‚úÖ
  - [x] **CLI-005a**: Implement proper indexing for tweet searches (user, date, keywords) using Drizzle
  - [x] **CLI-005b**: Add indexes for embedding similarity searches and vector operations
  - [x] **CLI-005c**: Optimize vector storage (consider separate table for large embeddings if needed)
  - [x] **CLI-005d**: Implement connection pooling for concurrent operations using Bun SQLite
  - [x] **CLI-005e**: Add query performance monitoring and logging for slow queries
  - [x] **CLI-005f**: Test with large datasets (50k+ tweets, 10k+ embeddings) for scalability
  - [x] **CLI-005g**: Benchmark SQLite vs JSON performance (read/write operations, memory usage)
  - [x] **CLI-005h**: Implement database vacuum and optimization routines for maintenance
  - [x] **CLI-005i**: Add database size monitoring and cleanup for old data
  - [x] **CLI-005j**: Test concurrent read/write operations under load using Bun's async capabilities
  - [x] **CLI-005k**: Create performance report with recommendations and benchmarks
  - **Files**: `src/database/optimization.ts` ‚úÖ, `benchmarks/sqlite-performance.ts` ‚úÖ, `src/cli.ts` ‚úÖ (optimize & benchmark commands)
  - **Dependencies**: CLI-004 ‚úÖ
  - **Acceptance**: SQLite performs 3x+ faster than JSON for large datasets, handles 50k+ tweets efficiently ‚úÖ

## Phase 2: Interactive Prompts (Priority: High)

### Completed Tasks

- [x] **PROMPT-001: Content type selection** ‚úÖ
  - [x] **PROMPT-001a**: Research Bun-compatible prompt libraries (@inquirer/prompts selected)
  - [x] **PROMPT-001b**: Install chosen prompt library: `bun add @inquirer/prompts`
  - [x] **PROMPT-001c**: Create `src/prompts/` directory structure
  - [x] **PROMPT-001d**: Create `src/prompts/contentType.ts` with content type selection logic
  - [x] **PROMPT-001e**: Implement radio button selection: "Tweets only", "Replies only", "Both"
  - [x] **PROMPT-001f**: Add input validation to ensure valid selection
  - [x] **PROMPT-001g**: Add default selection (Tweets only) for quick usage
  - [x] **PROMPT-001h**: Store user selection in session object for later use
  - [x] **PROMPT-001i**: Add help text explaining each option
  - [x] **PROMPT-001j**: Test prompt functionality and user experience
  - **Files**: `src/prompts/contentType.ts` ‚úÖ, `src/types/session.ts` ‚úÖ
  - **Dependencies**: CLI-001 ‚úÖ
  - **Acceptance**: User can interactively select content type with validation ‚úÖ

- [x] **PROMPT-002: Search scope selection** ‚úÖ
  - [x] **PROMPT-002a**: Create `src/prompts/searchScope.ts` with scope selection logic
  - [x] **PROMPT-002b**: Implement radio button selection: "All posts", "Keyword filtered"
  - [x] **PROMPT-002c**: Add conditional keyword input when "Keyword filtered" is selected
  - [x] **PROMPT-002d**: Implement keyword input with placeholder text and examples
  - [x] **PROMPT-002e**: Add comma-separated keyword parsing and validation
  - [x] **PROMPT-002f**: Trim whitespace and handle empty keywords
  - [x] **PROMPT-002g**: Show example format: "AI, programming, typescript"
  - [x] **PROMPT-002h**: Add keyword preview/confirmation before proceeding
  - [x] **PROMPT-002i**: Store keywords array in session object
  - [x] **PROMPT-002j**: Add option to go back and modify keywords
  - **Files**: `src/prompts/searchScope.ts` ‚úÖ, keyword parsing integrated
  - **Dependencies**: PROMPT-001 ‚úÖ
  - **Acceptance**: User can select scope and enter validated keywords ‚úÖ

- [x] **PROMPT-003: Time range filtering** ‚úÖ
  - [x] **PROMPT-003a**: Create `src/prompts/timeRange.ts` with time range selection
  - [x] **PROMPT-003b**: Create `src/utils/dateUtils.ts` with date calculation functions
  - [x] **PROMPT-003c**: Implement radio selection: "Last week", "Last month", "Last 3 months", etc.
  - [x] **PROMPT-003d**: Add "Custom range" option with date input prompts
  - [x] **PROMPT-003e**: Calculate start/end dates for each predefined range
  - [x] **PROMPT-003f**: Validate custom date inputs (format, logical order)
  - [x] **PROMPT-003g**: Display calculated date range for user confirmation
  - [x] **PROMPT-003h**: Store date range in session object (startDate, endDate)
  - [x] **PROMPT-003i**: Add timezone handling for accurate date filtering
  - [x] **PROMPT-003j**: Test date calculations with various timezones
  - **Files**: `src/prompts/timeRange.ts` ‚úÖ, `src/utils/dateUtils.ts` ‚úÖ, `src/types/session.ts` ‚úÖ
  - **Dependencies**: PROMPT-002 ‚úÖ
  - **Acceptance**: User can select time ranges with accurate date calculations ‚úÖ

## Phase 3: Enhanced Scraping (Priority: Medium) ‚úÖ COMPLETED

### Completed Tasks

- [x] **SCRAPE-001: Implement content filtering** ‚úÖ
  - [x] **SCRAPE-001a**: Create filtering logic integrated in scrape command ‚úÖ
  - [x] **SCRAPE-001b**: Implement reply filtering based on `includeReplies` flag ‚úÖ
  - [x] **SCRAPE-001c**: Implement retweet filtering based on `includeRetweets` flag ‚úÖ
  - [x] **SCRAPE-001d**: Create keyword matching algorithm in `src/prompts/searchScope.ts` ‚úÖ
  - [x] **SCRAPE-001e**: Implement keyword filtering during tweet processing ‚úÖ
  - [x] **SCRAPE-001f**: Add filter statistics (content filtered, keyword filtered counts) ‚úÖ
  - [x] **SCRAPE-001g**: Update scrape command to use session filters from interactive prompts ‚úÖ
  - [x] **SCRAPE-001h**: Add filter display showing active filters before scraping ‚úÖ
  - [x] **SCRAPE-001i**: Test filtering with various content types and keywords ‚úÖ
  - **Files**: `src/commands/scrape.ts` ‚úÖ, `src/prompts/searchScope.ts` ‚úÖ
  - **Dependencies**: PROMPT-003 ‚úÖ
  - **Acceptance**: Scraping accurately filters content based on user preferences ‚úÖ

- [x] **SCRAPE-002: Time-based filtering** ‚úÖ
  - [x] **SCRAPE-002a**: Create date filtering logic in `src/utils/dateUtils.ts` ‚úÖ
  - [x] **SCRAPE-002b**: Implement tweet date parsing and validation ‚úÖ
  - [x] **SCRAPE-002c**: Add date range filtering during scraping process using `isWithinDateRange()` ‚úÖ
  - [x] **SCRAPE-002d**: Optimize filtering during tweet iteration ‚úÖ
  - [x] **SCRAPE-002e**: Add progress indicators showing date progress ‚úÖ
  - [x] **SCRAPE-002f**: Handle timezone differences in tweet timestamps ‚úÖ
  - [x] **SCRAPE-002g**: Add date filtering statistics and reporting ‚úÖ
  - [x] **SCRAPE-002h**: Test with various date ranges and timezones ‚úÖ
  - [x] **SCRAPE-002i**: Add fallback for tweets with missing/invalid dates ‚úÖ
  - **Files**: `src/utils/dateUtils.ts` ‚úÖ, `src/commands/scrape.ts` ‚úÖ
  - **Dependencies**: SCRAPE-001 ‚úÖ
  - **Acceptance**: Only tweets within specified time range are scraped efficiently ‚úÖ

- [x] **SCRAPE-003: Duplicate detection** ‚úÖ
  - [x] **SCRAPE-003a**: Implement duplicate detection logic in scrape command ‚úÖ
  - [x] **SCRAPE-003b**: Implement tweet ID-based duplicate detection using `tweetExists()` ‚úÖ
  - [x] **SCRAPE-003c**: Handle UNIQUE constraint errors gracefully ‚úÖ
  - [x] **SCRAPE-003d**: Create database UNIQUE constraints on tweet IDs in schema ‚úÖ
  - [x] **SCRAPE-003e**: Add duplicate statistics and reporting ‚úÖ
  - [x] **SCRAPE-003f**: Handle edge cases with try-catch error handling ‚úÖ
  - [x] **SCRAPE-003g**: Create duplicate resolution strategies (skip duplicates) ‚úÖ
  - [x] **SCRAPE-003h**: Test with datasets containing known duplicates ‚úÖ
  - [x] **SCRAPE-003i**: Add duplicate detection results in scraping output ‚úÖ
  - **Files**: `src/commands/scrape.ts` ‚úÖ, `src/database/queries.ts` ‚úÖ, `src/database/schema.ts` ‚úÖ
  - **Dependencies**: CLI-003 ‚úÖ
  - **Acceptance**: No duplicate tweets in database, comprehensive detection ‚úÖ

## Phase 4: Performance & Reliability (Priority: HIGH) üö® CRITICAL

**‚ö†Ô∏è PRODUCTION RELIABILITY FOCUS**: Since we're using users' auth tokens, we MUST implement proper rate limiting to prevent account bans and ensure production reliability.

### Next Priority Tasks

- [ ] **PERF-002: Rate limiting & Auth Token Protection** üö® CRITICAL
  - **PERF-002a**: Research X/Twitter rate limits for scraping operations
    - Free tier: 1 request/15min for user timeline (`GET /2/users/:id/tweets`)
    - Basic tier: 5 requests/15min for user timeline
    - Pro tier: 900 requests/15min for user timeline
    - **CRITICAL**: Understand that we're using cookie-based scraping, not official API
  - **PERF-002b**: Create `src/rateLimit/` directory structure
  - **PERF-002c**: Create `src/rateLimit/manager.ts` with rate limiting logic
  - **PERF-002d**: Implement token bucket algorithm for rate limiting
  - **PERF-002e**: Add configurable rate limits based on detected account tier
  - **PERF-002f**: Implement exponential backoff for failed requests (start 1s, max 300s)
  - **PERF-002g**: Add retry logic with jitter to avoid thundering herd
  - **PERF-002h**: Handle X/Twitter rate limit responses (429, 503 errors)
  - **PERF-002i**: Add rate limit status monitoring and logging
  - **PERF-002j**: Implement graceful degradation when rate limited
  - **PERF-002k**: Add user notifications for rate limit delays with ETA
  - **PERF-002l**: Estimate tweet collection based on rate limits
  - **PERF-002m**: Add safety mechanisms to prevent account suspension
  - **PERF-002n**: Test rate limiting with high-volume scraping scenarios
  - **PERF-002o**: Create rate limit configuration options
  - **Files**: `src/rateLimit/manager.ts`, `src/rateLimit/config.ts`, `src/utils/backoff.ts`, `src/rateLimit/estimator.ts`
  - **Dependencies**: SCRAPE-003 ‚úÖ
  - **Acceptance**: Tool respects all rate limits, protects user accounts, provides accurate estimates

- [ ] **PERF-001: Caching system** (Lower priority after rate limiting)
  - **PERF-001a**: Create `src/cache/` directory structure
  - **PERF-001b**: Create `src/cache/manager.ts` with cache management logic
  - **PERF-001c**: Implement file-based caching using Bun's file system APIs
  - **PERF-001d**: Create cache key generation for tweets and embeddings
  - **PERF-001e**: Implement TTL (time-to-live) for cache entries
  - **PERF-001f**: Create `src/cache/strategies.ts` with different caching strategies
  - **PERF-001g**: Implement LRU (Least Recently Used) cache eviction
  - **PERF-001h**: Add cache size limits and cleanup mechanisms
  - **PERF-001i**: Cache tweet data to avoid re-scraping recent content
  - **PERF-001j**: Cache embeddings to avoid regenerating vectors
  - **PERF-001k**: Add cache statistics and monitoring
  - **PERF-001l**: Create cache invalidation for stale data
  - **PERF-001m**: Test cache performance with large datasets
  - **PERF-001n**: Add cache configuration options
  - **Files**: `src/cache/manager.ts`, `src/cache/strategies.ts`, `src/cache/config.ts`
  - **Dependencies**: CLI-003 ‚úÖ
  - **Acceptance**: Caching reduces API calls and improves performance significantly

- [ ] **PERF-003: Progress indicators**
  - **PERF-003a**: Create `src/ui/` directory structure
  - **PERF-003b**: Create `src/ui/progress.ts` with progress bar components
  - **PERF-003c**: Install Bun-compatible progress bar library
  - **PERF-003d**: Implement progress bars for tweet scraping operations
  - **PERF-003e**: Add progress indicators for embedding generation
  - **PERF-003f**: Show time estimates based on current progress
  - **PERF-003g**: Display completion percentages and ETA
  - **PERF-003h**: Add operation status messages (scraping, processing, etc.)
  - **PERF-003i**: Implement multi-step progress for complex operations
  - **PERF-003j**: Add progress persistence for resumable operations
  - **PERF-003k**: Create spinner animations for indeterminate progress
  - **PERF-003l**: Test progress indicators with various operation sizes
  - **Files**: `src/ui/progress.ts`, `src/ui/spinner.ts`, `src/ui/status.ts`
  - **Dependencies**: CLI-001
  - **Acceptance**: Users see clear, accurate progress for all long operations

## Phase 5: User Experience (Priority: Low)

### Future Tasks

- [ ] **UX-001: Configuration system**
  - **UX-001a**: Create `src/config/` directory structure
  - **UX-001b**: Create `src/config/manager.ts` with configuration management
  - **UX-001c**: Design configuration schema with default values
  - **UX-001d**: Implement user config directory creation (`~/.xgpt/`)
  - **UX-001e**: Create `config.json` file with user preferences
  - **UX-001f**: Add config commands: `twtgpt config set <key> <value>`
  - **UX-001g**: Add config commands: `twtgpt config get <key>`
  - **UX-001h**: Add config commands: `twtgpt config list`
  - **UX-001i**: Add config commands: `twtgpt config reset`
  - **UX-001j**: Implement environment-specific config overrides
  - **UX-001k**: Add config validation and type checking
  - **UX-001l**: Create config migration system for updates
  - **UX-001m**: Test config persistence and loading
  - **UX-001n**: Add config backup and restore functionality
  - **Files**: `src/config/manager.ts`, `src/config/schema.ts`, `src/commands/config.ts`
  - **Dependencies**: CLI-001
  - **Acceptance**: Users can save, modify, and reuse all preferences

- [ ] **UX-002: Enhanced error handling**
  - **UX-002a**: Create `src/errors/` directory structure
  - **UX-002b**: Create `src/errors/types.ts` with custom error classes
  - **UX-002c**: Create `src/errors/handler.ts` with centralized error handling
  - **UX-002d**: Implement user-friendly error messages for common issues
  - **UX-002e**: Add error codes and categorization system
  - **UX-002f**: Create error recovery mechanisms and suggestions
  - **UX-002g**: Add error logging with different severity levels
  - **UX-002h**: Implement graceful degradation for non-critical errors
  - **UX-002i**: Add error reporting and debugging information
  - **UX-002j**: Create error handling for network issues
  - **UX-002k**: Add error handling for authentication problems
  - **UX-002l**: Test error scenarios and recovery paths
  - **Files**: `src/errors/handler.ts`, `src/errors/types.ts`, `src/errors/recovery.ts`
  - **Dependencies**: CLI-002
  - **Acceptance**: All errors provide helpful messages and recovery options

- [ ] **UX-003: Export options**
  - **UX-003a**: Create `src/export/` directory structure
  - **UX-003b**: Create `src/export/formats.ts` with export format handlers
  - **UX-003c**: Implement JSON export with customizable formatting
  - **UX-003d**: Implement CSV export with configurable columns
  - **UX-003e**: Implement Markdown export with readable formatting
  - **UX-003f**: Add XML export for structured data needs
  - **UX-003g**: Create `src/commands/export.ts` with export commands
  - **UX-003h**: Add export filtering options (date range, keywords, etc.)
  - **UX-003i**: Implement export templates for different use cases
  - **UX-003j**: Add export compression options (zip, gzip)
  - **UX-003k**: Create export scheduling and automation
  - **UX-003l**: Add export validation and integrity checks
  - **UX-003m**: Test exports with large datasets
  - **UX-003n**: Add export progress indicators
  - **Files**: `src/export/formats.ts`, `src/commands/export.ts`, `src/export/templates.ts`
  - **Dependencies**: CLI-003
  - **Acceptance**: Users can export data in multiple formats with full customization

## üö® CRITICAL: Production Reliability Implementation Plan

### Phase 4 Priority: Rate Limiting & Auth Token Protection

**Why This is Critical:**
- We're using users' personal X/Twitter auth tokens (AUTH_TOKEN, CT0)
- Aggressive scraping can lead to account suspension/bans
- X/Twitter has strict rate limits that must be respected
- Production reliability is essential when handling user credentials

**Rate Limit Research Findings:**
- **Cookie-based scraping**: Not official API, more restrictive
- **Estimated limits**: ~1-5 requests per minute for timeline scraping
- **Risk factors**: High-frequency requests, large batch sizes, concurrent sessions
- **Consequences**: Account suspension, IP blocking, token invalidation

**Implementation Strategy:**
1. **Conservative rate limiting**: Start with 1 request/30 seconds
2. **Adaptive throttling**: Adjust based on response patterns
3. **Error detection**: Monitor for 429, 503, and auth errors
4. **Graceful degradation**: Pause and retry with exponential backoff
5. **User communication**: Clear progress updates and wait time estimates

### Relevant Files

- `src/cli.ts` - Main CLI entry point ‚úÖ
- `src/commands/` - Command modules directory ‚úÖ
- `src/prompts/` - Interactive prompt modules ‚úÖ
- `src/database/` - Database schema and migrations ‚úÖ
- `src/rateLimit/` - Rate limiting system üö® NEW PRIORITY
- `src/cache/` - Caching system ‚è≥
- `src/config/` - Configuration management ‚è≥
- `src/ui/` - User interface components ‚è≥
- `src/utils/` - Utility functions ‚úÖ
- `package.json` - Updated with CLI binary configuration ‚úÖ

### Architecture Decisions

1. **CLI Framework**: Use a lightweight CLI library compatible with Bun (avoid Node.js-specific libraries)
2. **Database**: Bun's native `bun:sqlite` with Drizzle ORM for type-safe, performant data operations
3. **ORM**: Drizzle ORM for schema management, migrations, and type-safe queries
4. **Prompts**: Interactive CLI prompts with validation using Bun-compatible libraries
5. **Progress Indicators**: cli-progress for visual feedback during long operations
6. **Caching**: File-based caching with TTL support using Bun's file system APIs
7. **Error Handling**: Centralized error handling with user-friendly messages
8. **Configuration**: JSON-based config files in user home directory using Bun's built-in JSON support

### Bun-Specific Implementation Notes

- **Database**: Use `import { Database } from "bun:sqlite"` with Drizzle ORM wrapper
- **ORM**: Drizzle ORM provides type safety, migrations, and optimized queries for Bun SQLite
- **WAL Mode**: Enable Write-Ahead Logging for better concurrent performance
- **File Operations**: Use Bun's built-in `Bun.file()` and `Bun.write()` when possible
- **Process Management**: Use `Bun.$` for shell commands instead of external process libraries
- **Environment**: Leverage Bun's automatic .env loading (no dotenv needed)
- **CLI Libraries**: Choose libraries that work with Bun's runtime (test compatibility first)
- **Progress Bars**: Use cli-progress for visual feedback during scraping and processing

### üéØ NEXT IMPLEMENTATION: PERF-002 Rate Limiting

**Estimated Timeline: 1-2 days**

**Day 1: Core Rate Limiting Infrastructure**
1. **Research & Planning** (2 hours)
   - Analyze current scraping patterns in `src/commands/scrape.ts`
   - Research X/Twitter rate limit patterns and detection methods
   - Design rate limiting architecture

2. **Basic Rate Limiting** (4 hours)
   - Create `src/rateLimit/manager.ts` with token bucket algorithm
   - Create `src/rateLimit/config.ts` with conservative default limits
   - Integrate rate limiting into scrape command

3. **Error Handling** (2 hours)
   - Add detection for rate limit responses (429, 503, auth errors)
   - Implement exponential backoff with jitter
   - Add graceful error recovery

**Day 2: Advanced Features & Testing**
1. **User Experience** (3 hours)
   - Add progress indicators with rate limit awareness
   - Create tweet collection estimator based on rate limits
   - Add user notifications for delays and wait times

2. **Safety & Monitoring** (3 hours)
   - Add rate limit status monitoring and logging
   - Implement safety mechanisms to prevent account suspension
   - Add configuration options for different rate limit profiles

3. **Testing & Validation** (2 hours)
   - Test with various scraping scenarios
   - Validate rate limiting behavior
   - Document usage and safety guidelines

**Expected Outcome:**
- Production-ready rate limiting that protects user accounts
- Accurate estimates of scraping time based on rate limits
- Clear user communication about delays and progress
- Robust error handling and recovery mechanisms

### Development Workflow (Updated)

1. ‚úÖ Phase 1 (CLI Foundation) - COMPLETED
2. ‚úÖ Phase 2 (Interactive Prompts) - COMPLETED
3. ‚úÖ Phase 3 (Enhanced Scraping) - COMPLETED
4. üö® **Phase 4 (Performance & Reliability)** - CRITICAL NEXT STEP
5. ‚è≥ Phase 5 (UX) - Future improvements

### Testing Strategy

- Unit tests for utility functions and data processing
- Integration tests for CLI commands
- Manual testing for interactive prompts
- Performance testing for large datasets
